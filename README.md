# ğŸ¬ Albert Query - Agentic Movie Intelligence System

> An intelligent conversational agent for querying movie and TV series data using Retrieval Augmented Generation (RAG), multi-tool orchestration, and semantic search.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.51.0-FF4B4B.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/LangChain-1.0.3-00ADD8.svg)](https://www.langchain.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-1.0.2-00ADD8.svg)](https://langchain-ai.github.io/langgraph/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-1.3.4-FF6F61.svg)](https://www.trychroma.com/)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Components](#components)
- [Workflow](#workflow)
- [Future Improvements](#future-improvements)
- [Contributors](#contributors)
- [License](#license)

---

## ğŸ¯ Overview

**Albert Query** is an agentic AI system we developed as part of our M1 project at **Albert School** in collaboration with **Mines Paris - PSL**. The system intelligently answers questions about movies and TV series by orchestrating multiple data sources and tools through a LangGraph-based workflow.

### What Makes It Special?

Unlike traditional chatbots, Albert Query:
- **Plans before acting** - Analyzes each question to determine which tools are needed
- **Multi-source intelligence** - Combines SQL databases, vector search, external APIs, and web search
- **Semantic understanding** - Uses OpenAI embeddings to find movies by plot similarity
- **Source attribution** - Always shows where information comes from
- **Context-aware** - Maintains conversation history for follow-up questions

### Use Cases

- ğŸ” **Semantic Search**: "Find me movies about space exploration with AI themes"
- ğŸ“Š **Data Analysis**: "How many comedies were released on Netflix after 2020?"
- ğŸ­ **Movie Discovery**: "Show me films similar to Inception"
- ğŸ“ˆ **Trend Analysis**: "What are the top-rated action movies from the 2010s?"
- ğŸŒ **Latest Info**: "What's trending in movies this week?"

---

## âœ¨ Features

### Core Capabilities

ğŸ§  **Intelligent Query Planning**
- LLM-based planner analyzes questions and conversation history
- Automatically selects optimal tools (SQL, Semantic Search, OMDB, Web)
- Avoids unnecessary API calls for efficiency

ğŸ—„ï¸ **Multi-Database SQL Queries**
- Comprehensive catalog of 8,000+ movies/shows from Netflix, Disney+, Amazon Prime
- Structured queries with filters (year, genre, rating, type)
- Automatic database schema understanding

ğŸ” **Semantic Vector Search**
- 114MB of OpenAI embeddings (text-embedding-3-small)
- Find movies by plot descriptions, themes, or similarity
- Natural language queries in English or French

ğŸ¬ **OMDB API Integration**
- Enriched movie metadata (actors, awards, ratings, posters)
- IMDb links for detailed information
- Full plot summaries

ğŸŒ **Web Search**
- DuckDuckGo integration for trending topics
- Latest movie news and releases
- Current events in cinema

ğŸ“š **Source Attribution**
- Tracks all information sources
- Visual display of databases, APIs, and search results used
- Clickable links to IMDb and search results

ğŸ’¬ **Conversational Memory**
- LangGraph checkpointing with MemorySaver
- Full conversation history maintained
- Context resolution ("that movie" â†’ actual movie name)

---

## ğŸ—ï¸ Architecture

Our system follows an **agentic architecture** using LangGraph to create a stateful, multi-tool workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER QUESTION                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  PLANNER NODE  â”‚ â† Analyzes question + history
            â”‚   (GPT-4o)     â”‚   Decides which tools needed
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ROUTER (cond.) â”‚ â† Routes to needed tools only
            â””â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”˜
                â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
      â”‚            â”‚           â”‚            â”‚
      â–¼            â–¼           â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQL    â”‚ â”‚ SEMANTIC â”‚ â”‚  OMDB    â”‚ â”‚   WEB    â”‚
â”‚  NODE    â”‚ â”‚   NODE   â”‚ â”‚  NODE    â”‚ â”‚  NODE    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  SYNTHESIZER   â”‚ â† Combines all results
         â”‚   (GPT-4o)     â”‚   Generates natural response
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ FINAL RESPONSE â”‚
         â”‚  + SOURCES     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Agentic_Systems_with_RAG_Lamy-Waerniers/
â”‚
â”œâ”€â”€ code/                                    # Main source code (modular architecture)
â”‚   â”œâ”€â”€ app.py                               # Streamlit UI and main entry point
â”‚   â”œâ”€â”€ agent.py                             # LangGraph workflow construction
â”‚   â”œâ”€â”€ nodes.py                             # All workflow nodes (planner, SQL, semantic, OMDB, web, synthesizer)
â”‚   â”œâ”€â”€ tools.py                             # Tool implementations (SQL query, web search, OMDB API, semantic search)
â”‚   â”œâ”€â”€ models.py                            # Shared type definitions (AgentState, PlannerOutput, SQLOutput)
â”‚   â”œâ”€â”€ config.py                            # Centralized configuration (API keys, paths, LLM instance)
â”‚   â”œâ”€â”€ utils.py                             # Helper functions (catalog builder, routing logic)
â”‚   â”œâ”€â”€ embedding.py                         # Vector embedding utilities
â”‚   â””â”€â”€ notebooks/                           # Jupyter notebooks for development
â”‚       â”œâ”€â”€ embeding.ipynb                   # Embedding pipeline notebook
â”‚       â”œâ”€â”€ SQLdb_creator.ipynb              # Database creation from CSVs
â”‚       â””â”€â”€ test_semantic_search.ipynb       # Semantic search validation
â”‚
â”œâ”€â”€ data/                                    # Data storage
â”‚   â”œâ”€â”€ csv_db/                              # Source CSV files
â”‚   â”‚   â”œâ”€â”€ amazon_prime_titles.csv          # Amazon Prime catalog (3.9MB)
â”‚   â”‚   â”œâ”€â”€ netflix_titles.csv               # Netflix catalog (3.4MB)
â”‚   â”‚   â””â”€â”€ disney_plus_titles.csv           # Disney+ catalog (385KB)
â”‚   â”œâ”€â”€ databases/
â”‚   â”‚   â””â”€â”€ movie.db                         # Consolidated SQLite DB (32.9MB)
â”‚   â”œâ”€â”€ vector_database/                     # ChromaDB persistent storage
â”‚   â”‚   â”œâ”€â”€ chroma.sqlite3                   # Vector DB metadata (42.7MB)
â”‚   â”‚   â””â”€â”€ 19c0759d-.../                    # Embedding data (114MB)
â”‚   â””â”€â”€ memory/                              # Conversation storage
â”‚       â”œâ”€â”€ conversations/
â”‚       â””â”€â”€ user_profiles/
â”‚
â”œâ”€â”€ doc/                                     # Documentation
â”‚   â”œâ”€â”€ graph_schema.png                     # LangGraph workflow diagram
â”‚   â”œâ”€â”€ omdb_api_doc.json                    # OMDB API reference
â”‚   â””â”€â”€ OMDB_API_doc.txt
â”‚
â”œâ”€â”€ .env                                     # Environment configuration (git-ignored)
â”œâ”€â”€ .gitignore                               # Git ignore rules
â”œâ”€â”€ requirements.txt                         # Python dependencies (223 packages)
â””â”€â”€ README.md                                # This file
```

### ğŸ›ï¸ Architecture Breakdown

The codebase follows a **modular architecture** to avoid circular imports and improve maintainability:

#### Core Modules:

- **`config.py`** - Central configuration hub
  - API keys (OpenAI, OMDB)
  - Absolute paths to data folders
  - LLM instance (ChatOpenAI)

- **`models.py`** - Shared type definitions
  - `AgentState`: TypedDict defining the workflow state
  - `PlannerOutput`: Pydantic model for planner decisions
  - `SQLOutput`: Pydantic model for SQL execution decisions

- **`tools.py`** - Tool implementations
  - `execute_sql_query()`: Query SQLite databases
  - `semantic_search()`: Vector similarity search with ChromaDB
  - `omdb_api()`: Fetch movie metadata from OMDB
  - `web_search()`: DuckDuckGo web search

- **`nodes.py`** - LangGraph workflow nodes
  - `planner_node()`: Analyzes question and decides which tools to use
  - `sql_node()`: Generates and executes SQL queries
  - `semantic_search_node()`: Performs vector search
  - `omdb_node()`: Fetches enriched movie data
  - `web_node()`: Searches the web
  - `synthesizer_node()`: Combines results into natural language response

- **`utils.py`** - Helper functions
  - `build_db_catalog()`: Introspects database schema
  - `format_catalog_for_llm()`: Formats catalog for LLM prompts
  - Routing functions for conditional edges

- **`agent.py`** - LangGraph workflow builder
  - Constructs the StateGraph
  - Defines node connections and routing
  - Compiles workflow with MemorySaver checkpointer

- **`app.py`** - Streamlit application
  - UI components (chat interface, source attribution)
  - Session state management
  - Workflow execution and streaming

This modular design ensures:
- âœ… No circular import dependencies
- âœ… Clear separation of concerns
- âœ… Easy testing and maintenance
- âœ… Reusable components

---

## ğŸ§© Components

### 1. Workflow Nodes (`nodes.py`)

The core workflow nodes built with **LangGraph**.

**Key Components:**

#### Planner Node
```python
def planner_node(state: AgentState) -> dict:
    """Analyzes question and plans tool usage"""
    # Uses GPT-4o-mini to decide which tools are needed
    # Returns: needs_sql, needs_semantic, needs_omdb, needs_web flags
    # Also prepares specific queries for each tool
```

**Decision Logic:**
- SQL: For structured data (titles, years, ratings, genres)
- Semantic: For plot-based searches and "movies like X"
- OMDB: For detailed metadata (actors, awards, posters)
- Web: For trending topics and recent news

#### SQL Node
```python
def sql_node(state: AgentState) -> dict:
    """Generates and executes SQL queries"""
    # Analyzes database catalog
    # Generates SQL with proper table names, filters
    # Executes query and returns results
```

**Features:**
- Auto-detects correct table names
- Handles genre filtering (comma-separated values)
- Year ranges, type filtering (Movie vs TV Show)
- Result limiting and ordering

#### Semantic Search Node
```python
def semantic_search_node(state: AgentState) -> dict:
    """Performs vector similarity search"""
    # Translates query to English if needed
    # Optimizes query for embeddings
    # Searches ChromaDB with OpenAI embeddings
    # Returns top-k similar movies
```

**Important Notes:**
- Movie descriptions are in **English** - queries are auto-translated
- Similarity scores may appear low (40-60%) but rankings are accurate
- Uses cosine similarity on text-embedding-3-small vectors

#### OMDB Node
```python
def omdb_node(state: AgentState) -> dict:
    """Fetches enriched movie data from OMDB API"""
    # Extracts movie title from context
    # Queries OMDB API
    # Returns: plot, actors, awards, IMDb rating, poster URL
```

#### Web Search Node
```python
def web_node(state: AgentState) -> dict:
    """Performs web search via DuckDuckGo"""
    # Executes search query
    # Returns recent results with URLs
```

#### Synthesizer Node
```python
def synthesizer_node(state: AgentState) -> dict:
    """Combines all results into natural response"""
    # Takes SQL, semantic, OMDB, web results
    # Generates coherent natural language answer
    # Includes source attribution
```

### 2. Tools (`tools.py`)

Implements all the tools that the agent can use for data retrieval.

**Tool Functions:**

```python
@tool
def execute_sql_query(query: str, db_name: str, state_catalog: dict) -> str:
    """Execute SQL query on specified database"""
    # Validates database exists in catalog
    # Connects to SQLite database
    # Executes query and returns JSON results

@tool
def semantic_search(query: str, n_results: int = 5, table_filter: str = None) -> str:
    """Execute semantic search on movie embeddings"""
    # Connects to ChromaDB collection
    # Embeds query using OpenAI embeddings
    # Searches vector database for similar movies
    # Returns top-k results with similarity scores

@tool
def omdb_api(by: str = "title", t: str = None, plot: str = "full") -> str:
    """Query OMDb API for movie details"""
    # Queries OMDB with API key
    # Returns detailed movie information

@tool
def web_search(query: str, num_results: int = 5) -> str:
    """Web search via DuckDuckGo"""
    # Executes web search
    # Returns search results
```

**Embedding Structure (ChromaDB):**
```python
{
    "id": "net0042",                    # Unique ID (prefix + index)
    "document": "A sci-fi thriller...", # Movie description (embedded)
    "metadata": {
        "title": "The Matrix",
        "database": "movie.db",
        "table": "netflix_titles"
    }
}
```

### 3. Database Schema

**SQL Tables:**
Each table (netflix_titles, amazon_prime_titles, disney_plus_titles) contains:

| Column | Type | Description |
|--------|------|-------------|
| show_id | TEXT | Unique identifier (e.g., "s1") |
| type | TEXT | "Movie" or "TV Show" |
| title | TEXT | Movie/show title |
| director | TEXT | Director name(s) |
| cast | TEXT | Comma-separated actors |
| country | TEXT | Production country |
| date_added | TEXT | Date added to platform |
| release_year | INTEGER | Year of release |
| rating | TEXT | Age rating (PG, R, etc.) |
| duration | TEXT | Runtime or seasons |
| listed_in | TEXT | Comma-separated genres |
| description | TEXT | Plot summary |

**Total Records:** ~8,800 movies/shows

### 4. Vector Database

**ChromaDB Collection:**
- Name: `movie_descriptions`
- Embedding Model: OpenAI text-embedding-3-small (1536 dimensions)
- Total Vectors: ~8,800 movie descriptions
- Storage: Persistent on disk (~156MB total)
- Metadata: title, database, table for filtering

---

## ğŸ”„ Workflow

```
User Question
     â”‚
     â–¼
[Planner Node]
  Analyzes: question + history
  Decides: which tools needed
  Outputs: needs_sql, needs_semantic, needs_omdb, needs_web
     â”‚
     â–¼
[Conditional Router]
  Routes to first needed tool
     â”‚
     â”œâ”€â†’ [SQL Node] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”œâ”€â†’ [Semantic Node] â”€â”€â”€â”€â”€â”¤
     â”œâ”€â†’ [OMDB Node] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â””â”€â†’ [Web Node] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚
                              â–¼
                    [Synthesizer Node]
                    Combines all results
                    Generates response
                              â”‚
                              â–¼
                    [Final Response + Sources]
```

## ğŸš§ Future Improvements

We've identified several areas for optimization and enhancement. Here's our roadmap:

### 1. ğŸ“¦ **Catalog Caching System**

**Problem:**
- Database catalog is rebuilt on every app startup
- Slow initialization (~2-5 seconds)
- Redundant SQL queries for schema introspection

**Solution:**
```python
# Implement caching with invalidation detection
def get_or_build_catalog(db_path: str, cache_path: str) -> dict:
    """
    Cache database catalog as JSON
    - Compare file modification times to detect changes
    - Load from cache if DB unchanged
    - Rebuild only when necessary
    """
```

**Expected Impact:**
- âš¡ 10-50x faster startup time
- ğŸ’¾ Reduced SQL queries
- ğŸ”„ Auto-invalidation on schema changes

**Implementation:**
- Save catalog to `data/databases/catalog_cache.json`
- Include DB file mtime and size for change detection
- Add force-rebuild option for manual invalidation

### 2. ğŸ§  **Persistent Long-Term Memory**

**Current State:**
- Memory stored in LangGraph's MemorySaver (in-memory only)
- Lost on application restart
- No cross-session learning

**Proposed Architecture:**
```python
# SQLite-based conversation storage
conversations/
  â”œâ”€â”€ user_123/
  â”‚   â”œâ”€â”€ session_20250116_001.json    # Conversation history
  â”‚   â”œâ”€â”€ session_20250116_002.json
  â”‚   â””â”€â”€ preferences.json              # Learned preferences
  â””â”€â”€ user_456/
      â””â”€â”€ ...

# Conversation schema
{
  "session_id": "20250116_001",
  "user_id": "user_123",
  "timestamp": "2025-01-16T10:30:00Z",
  "messages": [...],
  "topics": ["action movies", "2020s cinema"],
  "preferences_learned": {
    "favorite_genres": ["action", "sci-fi"],
    "preferred_platforms": ["netflix"]
  }
}
```

**Features to Add:**
- ğŸ’¾ Persist conversations to disk (JSON or SQLite)
- ğŸ‘¤ User-specific history and preferences
- ğŸ” Semantic search over past conversations
- ğŸ“Š Analytics on user interests
- ğŸ¯ Personalized recommendations based on history

**Technical Implementation:**
- Replace MemorySaver with custom SQLiteCheckpointer
- Add user authentication (see #3)
- Implement conversation summarization for long histories
- Privacy controls (GDPR compliance)

### 3. ğŸ” **User Management & API Key Interface**

**Current Limitation:**
- Single shared API keys in `.env`
- No multi-user support
- API costs not attributable to users

**Proposed UI:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš™ï¸ Settings                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User ID: [user_123]                    â”‚
â”‚                                         â”‚
â”‚  ğŸ”‘ OpenAI API Key:                    â”‚
â”‚  [sk-proj-******************] [Save]    â”‚
â”‚                                         â”‚
â”‚  ğŸ¬ OMDB API Key:                      â”‚
â”‚  [8871ab87           ] [Save]           â”‚
â”‚                                         â”‚
â”‚  ğŸ’° Token Usage This Session: 1,234    â”‚
â”‚  ğŸ’µ Estimated Cost: $0.05              â”‚
â”‚                                         â”‚
â”‚  [Clear History] [Export Data]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**
- Streamlit sidebar with settings panel
- Encrypted key storage per user (keyring library)
- Session-based authentication
- Token tracking and cost estimation
- Rate limiting per user

### 4. ğŸ”„ **Workflow Enhancement: Planner Loop**

**Current Issue:**
- Linear workflow: Planner â†’ Tools â†’ Synthesizer â†’ End
- No feedback loop if initial plan was insufficient
- Cannot self-correct or ask for more tools

**Proposed Architecture:**

```
User Question
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLANNER       â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
     â”‚                        â”‚
     â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  TOOL EXECUTOR â”‚            â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
     â”‚                        â”‚
     â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  EVALUATOR     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (New Node)    â”‚ If insufficient data, loop back
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ If sufficient
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYNTHESIZER   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Evaluator Node Logic:**
```python
def evaluator_node(state: AgentState) -> dict:
    """
    Checks if gathered data is sufficient to answer question
    Returns: continue (to synthesizer) or replan (back to planner)
    """
    results_quality = assess_data_completeness(state)

    if results_quality["sufficient"]:
        return {"next": "synthesize"}
    else:
        # Request additional tools
        return {
            "next": "planner",
            "feedback": "Need more data: " + results_quality["missing"]
        }
```

**Benefits:**
- ğŸ” Self-correcting behavior
- ğŸ¯ More complete answers
- ğŸ§  Adaptive tool selection
- âš¡ Avoids premature responses

### 5. ğŸ“ˆ **Embedding Quality Improvements**

**Current Limitations:**
- Similarity scores often low (<50%)
- Movie descriptions are single sentences only
- No chunking strategy
- Basic embedding model (text-embedding-3-small)

**Improvement Strategies:**

#### A. **Enhance Movie Description Quality** (Priority #1)
Currently, we only embed the plot description field from databases (single sentence).

**Solution:**
Use APIs to enrich our database with way longer movie descritions.
Expected Impact:
- ğŸ¯ More contextual embeddings
- ğŸ“ˆ Better similarity scores (10-20% improvement)
- ğŸ” Genre/cast matching in semantic search

### 6. ğŸ“‹ **Structured Output Enforcement**

**Problem:**
- JSON parsing errors possible in synthesizer
- Inconsistent response formats
- Difficult to extract structured data

**Solution - Pydantic Models:**

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class MovieResult(BaseModel):
    title: str
    year: int
    rating: Optional[str]
    genres: List[str]
    description: str
    source: str  # "sql", "semantic", "omdb"

class QueryResponse(BaseModel):
    answer: str = Field(..., description="Natural language answer")
    movies: List[MovieResult] = Field(default=[], description="Structured movie data")
    sources: List[str] = Field(..., description="Sources used")
    confidence: float = Field(..., ge=0, le=1, description="Confidence score")
    follow_up_suggestions: List[str] = Field(default=[], description="Suggested follow-up questions")

# Use in synthesizer
structured_llm = llm.with_structured_output(QueryResponse)
response = structured_llm.invoke(synthesis_prompt)
```

**Benefits:**
- âœ… Guaranteed valid JSON
- âœ… Type safety
- âœ… Easier frontend integration
- âœ… Better error messages

### 7. ğŸ¯ **Token Optimization**

**Current Issues:**
- Prompts are verbose (500-800 tokens each)
- Full database catalog sent to planner (1000+ tokens)
- Conversation history grows unbounded

**Optimization Strategies:**

#### A. **Prompt Compression**

#### B. **Catalog Summarization**

#### C. **Conversation Summarization**

#### D. **Lazy Loading**

**Expected Savings:**
- ğŸ“‰ 60-70% reduction in prompt tokens
- ğŸ’° ~$0.02 â†’ $0.005 per query
- âš¡ Faster LLM responses

### 8. ğŸ¨ **UI/UX Enhancements**

**Proposed Features:**
- ğŸ“Š **Results Table View** - Toggle between chat and table display
- ğŸ¬ **Movie Cards** - Rich display with posters, ratings, cast
- ğŸ“ˆ **Query Statistics** - Show token usage, cost, response time
- ğŸŒ“ **Dark Mode** - Theme switching
- ğŸ“¥ **Export Results** - Download conversations as JSON/CSV
- ğŸ¤ **Voice Input** - Speech-to-text for queries
- ğŸŒ **Multi-language Support** - Full i18n for French/English
- ğŸ“± **Mobile Responsiveness** - Optimize for mobile devices
- âŒ¨ï¸ **Keyboard Shortcuts** - Power user features

### 9. ğŸ§ª **Testing & Quality Assurance**

**Current Gap:** No automated tests

**Coverage Goals:**
- Unit tests for each node
- Integration tests for workflow
- Performance benchmarks
- Regression tests for common queries

### 10. ğŸ”’ **Security & Privacy**

**Enhancements Needed:**
- ğŸ” API key encryption at rest
- ğŸš« Input sanitization (SQL injection prevention)
- ğŸ•µï¸ PII detection and redaction in conversations
- ğŸ“œ Audit logging for all queries
- ğŸ›¡ï¸ Rate limiting and abuse prevention
- ğŸ” HTTPS enforcement in production
- ğŸ‘ï¸ Content filtering for inappropriate queries

### 11. ğŸš€ **Performance & Scalability**

**Optimization Opportunities:**
- âš¡ **Async Tool Execution** - Run SQL, Semantic, OMDB in parallel
- ğŸ’¾ **Result Caching** - Cache common queries (Redis)
- ğŸ—œï¸ **Vector Index Optimization** - Use HNSW parameters tuning
- ğŸ“Š **Database Indexing** - Add indexes on common query columns
- ğŸ”„ **Connection Pooling** - Reuse DB connections
- â˜ï¸ **Deployment** - Docker + cloud hosting (AWS/GCP)
- ğŸ“¦ **CDN Integration** - Cache static assets

### 12. ğŸ“Š **Analytics & Monitoring**

**Tracking Metrics:**
- ğŸ“ˆ Query latency by tool type
- ğŸ’° Cost per query (token usage)
- ğŸ¯ Tool selection accuracy (planner effectiveness)
- ğŸ‘¥ User engagement metrics
- âŒ Error rates and types
- ğŸ” Most common queries and topics
- ğŸ“Š Semantic search quality metrics


## ğŸ‘¥ Contributors

This project was developed as part of our Master's degree at **Albert School X Mines Paris - PSL**.

**Team:**
- Vincent Lamy & Alexandre Waerniers

**Institution:**
- Albert School (Paris, France)
- Mines Paris - PSL

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

<div align="center">

**â­ If you found this project useful, please consider giving it a star! â­**

</div>
