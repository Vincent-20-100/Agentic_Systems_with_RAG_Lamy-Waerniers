# ğŸ¬ Albert Query - Agentic Movie Intelligence System

> An intelligent conversational agent for querying movie and TV series data using Retrieval Augmented Generation (RAG), multi-tool orchestration, and semantic search.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.51.0-FF4B4B.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/LangChain-1.0.3-00ADD8.svg)](https://www.langchain.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-1.0.2-00ADD8.svg)](https://langchain-ai.github.io/langgraph/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-1.3.4-FF6F61.svg)](https://www.trychroma.com/)


## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Monitoring with Langfuse](#monitoring-with-langfuse)
- [Features](#features)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Components](#components)
- [Workflow](#workflow)
- [Future Improvements](#future-improvements)
- [Contributors](#contributors)
- [License](#license)

---

## Overview

**Albert Query** is an agentic AI system we developed as part of our M1 project at **Albert School** in collaboration with **Mines Paris - PSL**. The system intelligently answers questions about movies and TV series by orchestrating multiple data sources and tools through a LangGraph-based workflow.

### What Makes It Special?

Unlike traditional chatbots, Albert Query:
- **Plans before acting** - Analyzes each question to determine which tools are needed
- **Multi-source intelligence** - Combines SQL databases, vector search, external APIs, and web search
- **Semantic understanding** - Uses OpenAI embeddings to find movies by plot similarity
- **Source attribution** - Always shows where information comes from
- **Context-aware** - Maintains conversation history for follow-up questions

### Use Cases

- ğŸ” **Semantic Search**: "Find me movies about space exploration with AI themes"
- ğŸ“Š **Data Analysis**: "How many comedies were released on Netflix after 2020?"
- ğŸ­ **Movie Discovery**: "Show me films similar to Inception"
- ğŸ“ˆ **Trend Analysis**: "What are the top-rated action movies from the 2010s?"
- ğŸŒ **Latest Info**: "What's trending in movies this week?"

---

## Installation

### Step 1: Clone the repo
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### Step 2: Create a virtual env
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# or
venv\Scripts\activate  # Windows
```

### Step 3: Install dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Configure environment variables
Create a `.env` file at the root and add:
```env
OPENAI_API_KEY="your_openai_api_key"
OMDB_API_KEY="your_omdb_api_key"
LANGFUSE_SECRET_KEY = "your_langfuse_secret_key"
LANGFUSE_PUBLIC_KEY = "your_langfuse_public_key"
```

### Step 5: Data files
You can use the jupyter notebooks (code/notebooks) to create the SQL and the cevtor database using the .csv files.

**OR**

Download from [this folder](https://drive.google.com/drive/folders/1Z1vqq8Q9Hw3VKBpmrqh6aiE6ee28mcCG?usp=drive_link) and place them in the `data/` folder. Don't change the file names.

### Step 6: Run the app
```bash
streamlit run code/streamlit_app.py
```

The app will open at `http://localhost:8501`

---

## Monitoring with Langfuse

The application includes integrated monitoring and observability through [Langfuse](https://langfuse.com/), allowing you to track and analyze LLM interactions in real-time.

### Setup

1. Create a free account at [cloud.langfuse.com](https://cloud.langfuse.com)
2. Generate API keys from your project settings
3. Add them to your `.env` file:
   ```env
   LANGFUSE_SECRET_KEY="sk-lf-..."
   LANGFUSE_PUBLIC_KEY="pk-lf-..."
   ```

### Features

The Langfuse integration automatically tracks:
- **LLM calls**: All GPT-4o-mini requests and responses
- **Token usage**: Input/output token counts per query
- **Latency**: Response times for each agent node
- **Conversation traces**: Full workflow execution from planner to synthesizer
- **Cost estimation**: Automatic cost tracking based on token usage

### Viewing Traces

Access your Langfuse dashboard at [cloud.langfuse.com](https://cloud.langfuse.com) to:
- View detailed traces of each user query
- Analyze tool selection patterns (SQL, semantic, OMDB, web)
- Monitor performance metrics and identify bottlenecks
- Debug errors and track edge cases

All traces are automatically organized by session ID for easy conversation tracking.

---
## Features

### Core Capabilities

ğŸ§  **Intelligent Query Planning**
- LLM-based planner analyzes questions and conversation history
- Automatically selects optimal tools (SQL, Semantic Search, OMDB, Web)
- Avoids unnecessary API calls for efficiency

ğŸ—„ï¸ **Multi-Database SQL Queries**
- Comprehensive catalog of 8,000+ movies/shows from Netflix, Disney+, Amazon Prime
- Structured queries with filters (year, genre, rating, type)
- Automatic database schema understanding

ğŸ” **Semantic Vector Search**
- 114MB of OpenAI embeddings (text-embedding-3-small)
- Find movies by plot descriptions, themes, or similarity
- Natural language queries in English or French

ğŸ¬ **OMDB API Integration**
- Enriched movie metadata (actors, awards, ratings, posters)
- IMDb links for detailed information
- Full plot summaries

ğŸŒ **Web Search**
- DuckDuckGo integration for trending topics
- Latest movie news and releases
- Current events in cinema

ğŸ“š **Source Attribution**
- Tracks all information sources
- Visual display of databases, APIs, and search results used
- Clickable links to IMDb and search results

ğŸ’¬ **Conversational Memory**
- LangGraph checkpointing with MemorySaver
- Full conversation history maintained
- Context resolution ("that movie" â†’ actual movie name)

---

## Architecture

Our system follows an **agentic architecture** using LangGraph to create a stateful, multi-tool workflow:

```mermaid
flowchart TB
    START(["**START**"]) --> planner["**Planner Node**<br>Analyze query<br>Choose the tools<br>Generate the queries"]
    planner -- "needs_sql=true" --> sql["**SQL Node**<br>Execute SQL query"]
    planner -- "needs_semantic=true" --> semantic["**Semantic Node**<br>Vector search"]
    planner -- "needs_omdb=true" --> omdb["**OMDB Node**<br>Fetch movie data"]
    planner -- "needs_web=true" --> web["**Web Node**<br>DuckDuckGo search"]
    planner -- "All flags=false" --> synthesize["**Synthesizer Node**<br>Generate response"]
    sql --> synthesize
    semantic --> synthesize
    omdb --> synthesize
    web --> synthesize
    synthesize --> END(["**END**"])
```

## Project Structure

```
Agentic_Systems_with_RAG_Lamy-Waerniers/
â”‚
â”œâ”€â”€ code/                                    # Main source code (modular architecture)
â”‚   â”œâ”€â”€ app.py                               # Streamlit UI and main entry point
â”‚   â”œâ”€â”€ agent.py                             # LangGraph workflow construction
â”‚   â”œâ”€â”€ nodes.py                             # All workflow nodes (planner, SQL, semantic, OMDB, web, synthesizer)
â”‚   â”œâ”€â”€ tools.py                             # Tool implementations (SQL query, web search, OMDB API, semantic search)
â”‚   â”œâ”€â”€ models.py                            # Shared type definitions (AgentState, PlannerOutput, SQLOutput)
â”‚   â”œâ”€â”€ config.py                            # Centralized configuration (API keys, paths, LLM instance)
â”‚   â”œâ”€â”€ utils.py                             # Helper functions (catalog builder, routing logic)
â”‚   â”œâ”€â”€ embedding.py                         # Vector embedding utilities
â”‚   â””â”€â”€ notebooks/                           # Jupyter notebooks for development
â”‚       â”œâ”€â”€ embeding.ipynb                   # Embedding pipeline notebook
â”‚       â”œâ”€â”€ SQLdb_creator.ipynb              # Database creation from CSVs
â”‚       â””â”€â”€ test_semantic_search.ipynb       # Semantic search validation
â”‚
â”œâ”€â”€ data/                                    # Data storage
â”‚   â”œâ”€â”€ csv_db/                              # Source CSV files
â”‚   â”‚   â”œâ”€â”€ amazon_prime_titles.csv          # Amazon Prime catalog (3.9MB)
â”‚   â”‚   â”œâ”€â”€ netflix_titles.csv               # Netflix catalog (3.4MB)
â”‚   â”‚   â””â”€â”€ disney_plus_titles.csv           # Disney+ catalog (385KB)
â”‚   â”œâ”€â”€ databases/
â”‚   â”‚   â””â”€â”€ movie.db                         # Consolidated SQLite DB (32.9MB)
â”‚   â”œâ”€â”€ vector_database/                     # ChromaDB persistent storage
â”‚   â”‚   â”œâ”€â”€ chroma.sqlite3                   # Vector DB metadata (42.7MB)
â”‚   â”‚   â””â”€â”€ 19c0759d-.../                    # Embedding data (114MB)
â”‚   â””â”€â”€ memory/                              # Conversation storage
â”‚       â”œâ”€â”€ conversations/
â”‚       â””â”€â”€ user_profiles/
â”‚
â”œâ”€â”€ doc/                                     # Documentation
â”‚   â”œâ”€â”€ graph_schema.png                     # LangGraph workflow diagram
â”‚   â”œâ”€â”€ omdb_api_doc.json                    # OMDB API reference
â”‚   â””â”€â”€ OMDB_API_doc.txt
â”‚
â”œâ”€â”€ .env                                     # Environment configuration (git-ignored)
â”œâ”€â”€ .gitignore                               # Git ignore rules
â”œâ”€â”€ requirements.txt                         # Python dependencies (223 packages)
â””â”€â”€ README.md                                # This file
```

### Architecture Breakdown

#### Core Modules:

- **`config.py`** - Central configuration hub
  - API keys (OpenAI, OMDB)
  - Absolute paths to data folders
  - LLM instance (ChatOpenAI)

- **`models.py`** - Shared type definitions
  - `AgentState`: TypedDict defining the workflow state
  - `PlannerOutput`: Pydantic model for planner decisions
  - `SQLOutput`: Pydantic model for SQL execution decisions

- **`tools.py`** - Tool implementations
  - `execute_sql_query()`: Query SQLite databases
  - `semantic_search()`: Vector similarity search with ChromaDB
  - `omdb_api()`: Fetch movie metadata from OMDB
  - `web_search()`: DuckDuckGo web search

- **`nodes.py`** - LangGraph workflow nodes
  - `planner_node()`: Analyzes question and decides which tools to use
  - `sql_node()`: Generates and executes SQL queries
  - `semantic_search_node()`: Performs vector search
  - `omdb_node()`: Fetches enriched movie data
  - `web_node()`: Searches the web
  - `synthesizer_node()`: Combines results into natural language response

- **`utils.py`** - Helper functions
  - `build_db_catalog()`: Introspects database schema
  - `format_catalog_for_llm()`: Formats catalog for LLM prompts
  - Routing functions for conditional edges

- **`agent.py`** - LangGraph workflow builder
  - Constructs the StateGraph
  - Defines node connections and routing
  - Compiles workflow with MemorySaver checkpointer

- **`app.py`** - Streamlit application
  - UI components (chat interface, source attribution)
  - Session state management
  - Workflow execution and streaming

---

## Future Improvements

We've identified several areas for optimization and enhancement. Here's our roadmap:

### 1. ğŸ“¦ **Catalog Caching System**

**Problem:**
- Database catalog is rebuilt on every app startup
- Slow initialization (~2-5 seconds)
- Redundant SQL queries for schema introspection

**Solution:**
```python
# Implement caching with invalidation detection
def get_or_build_catalog(db_path: str, cache_path: str) -> dict:
    """
    Cache database catalog as JSON
    - Compare file modification times to detect changes
    - Load from cache if DB unchanged
    - Rebuild only when necessary
    """
```

**Expected Impact:**
- âš¡ 10-50x faster startup time
- ğŸ’¾ Reduced SQL queries
- ğŸ”„ Auto-invalidation on schema changes

**Implementation:**
- Save catalog to `data/databases/catalog_cache.json`
- Include DB file mtime and size for change detection
- Add force-rebuild option for manual invalidation

### 2. ğŸ§  **Persistent Long-Term Memory**

**Current State:**
- Memory stored in LangGraph's MemorySaver (in-memory only)
- Lost on application restart
- No cross-session learning

**Proposed Architecture:**
```python
# SQLite-based conversation storage
conversations/
  â”œâ”€â”€ user_123/
  â”‚   â”œâ”€â”€ session_20250116_001.json    # Conversation history
  â”‚   â”œâ”€â”€ session_20250116_002.json
  â”‚   â””â”€â”€ preferences.json              # Learned preferences
  â””â”€â”€ user_456/
      â””â”€â”€ ...

# Conversation schema
{
  "session_id": "20250116_001",
  "user_id": "user_123",
  "timestamp": "2025-01-16T10:30:00Z",
  "messages": [...],
  "topics": ["action movies", "2020s cinema"],
  "preferences_learned": {
    "favorite_genres": ["action", "sci-fi"],
    "preferred_platforms": ["netflix"]
  }
}
```

**Features to Add:**
- ğŸ’¾ Persist conversations to disk (JSON or SQLite)
- ğŸ‘¤ User-specific history and preferences
- ğŸ” Semantic search over past conversations
- ğŸ“Š Analytics on user interests
- ğŸ¯ Personalized recommendations based on history

**Technical Implementation:**
- Replace MemorySaver with custom SQLiteCheckpointer
- Add user authentication (see #3)
- Implement conversation summarization for long histories
- Privacy controls (GDPR compliance)

### 3. ğŸ” **User Management & API Key Interface**

**Current Limitation:**
- Single shared API keys in `.env`
- No multi-user support
- API costs not attributable to users

**Implementation:**
- Streamlit sidebar with settings panel
- Encrypted key storage per user (keyring library)
- Session-based authentication
- Token tracking and cost estimation
- Rate limiting per user

### 4. ğŸ”„ **Workflow Enhancement: Planner Loop**

**Current Issue:**
- Linear workflow: Planner â†’ Tools â†’ Synthesizer â†’ End
- No feedback loop if initial plan was insufficient
- Cannot self-correct or ask for more tools

**Proposed Architecture:**

```
User Question
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLANNER       â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
     â”‚                        â”‚
     â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  TOOL EXECUTOR â”‚            â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
     â”‚                        â”‚
     â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  EVALUATOR     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (New Node)    â”‚ If insufficient data, loop back
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ If sufficient
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYNTHESIZER   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. ğŸ“ˆ **Embedding Quality Improvements**

**Current Limitations:**
- Similarity scores often low (<50%)
- Movie descriptions are single sentences only
- No chunking strategy
- Basic embedding model (text-embedding-3-small)

**Improvement Strategies:**

#### A. **Enhance Movie Description Quality** (Priority #1)
Currently, we only embed the plot description field from databases (single sentence).
Use APIs to enrich our database with way longer movie descritions.
Expected Impact:
- ğŸ¯ More contextual embeddings
- ğŸ“ˆ Better similarity scores (10-20% improvement)
- ğŸ” Genre/cast matching in semantic search

### 6. ğŸ“‹ **Structured Output Enforcement**

**Problem:**
- JSON parsing errors possible in synthesizer
- Inconsistent response formats
- Difficult to extract structured data

### 7. ğŸ¯ **Token Optimization**

**Current Issues:**
- Prompts are verbose (500-800 tokens each)
- Full database catalog sent to planner (1000+ tokens)
- Conversation history grows unbounded

**Optimization Strategies:**

#### A. **Prompt Compression**

#### B. **Catalog Summarization**

#### C. **Conversation Summarization**

#### D. **Lazy Loading**

### 8. ğŸ¨ **UI/UX Enhancements**

**Proposed Features:**
- ğŸ“Š **Results Table View** - Toggle between chat and table display
- ğŸ¬ **Movie Cards** - Rich display with posters, ratings, cast
- ğŸ“ˆ **Query Statistics** - Show token usage, cost, response time
- ğŸŒ“ **Dark Mode** - Theme switching
- ğŸ“¥ **Export Results** - Download conversations as JSON/CSV
- ğŸ¤ **Voice Input** - Speech-to-text for queries
- ğŸŒ **Multi-language Support** - Full i18n for French/English
- ğŸ“± **Mobile Responsiveness** - Optimize for mobile devices
- âŒ¨ï¸ **Keyboard Shortcuts** - Power user features

### 9. ğŸ§ª **Testing & Quality Assurance**

**Current Gap:** No automated tests

**Coverage Goals:**
- Unit tests for each node
- Integration tests for workflow
- Performance benchmarks
- Regression tests for common queries

### 10. ğŸ”’ **Security & Privacy**

**Enhancements Needed:**
- ğŸ” API key encryption at rest
- ğŸš« Input sanitization (SQL injection prevention)
- ğŸ•µï¸ PII detection and redaction in conversations
- ğŸ“œ Audit logging for all queries
- ğŸ›¡ï¸ Rate limiting and abuse prevention
- ğŸ” HTTPS enforcement in production
- ğŸ‘ï¸ Content filtering for inappropriate queries

### 11. ğŸš€ **Performance & Scalability**

**Optimization Opportunities:**
- âš¡ **Async Tool Execution** - Run SQL, Semantic, OMDB in parallel
- ğŸ’¾ **Result Caching** - Cache common queries (Redis)
- ğŸ—œï¸ **Vector Index Optimization** - Use HNSW parameters tuning
- ğŸ“Š **Database Indexing** - Add indexes on common query columns
- ğŸ”„ **Connection Pooling** - Reuse DB connections
- â˜ï¸ **Deployment** - Docker + cloud hosting (AWS/GCP)
- ğŸ“¦ **CDN Integration** - Cache static assets

### 12. ğŸ“Š **Analytics & Monitoring**

**Tracking Metrics:**
- ğŸ“ˆ Query latency by tool type
- ğŸ’° Cost per query (token usage)
- ğŸ¯ Tool selection accuracy (planner effectiveness)
- ğŸ‘¥ User engagement metrics
- âŒ Error rates and types
- ğŸ” Most common queries and topics
- ğŸ“Š Semantic search quality metrics


## Contributors

This project was developed as part of our Master's degree at **Albert School X Mines Paris - PSL**.

**Team:**
- Vincent Lamy & Alexandre Waerniers

**Institution:**
- Albert School  X  Mines Paris PSL
(Paris - FRANCE)

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---

<div align="center">

**â­ If you found this project useful, please consider giving it a star! â­**

</div>
